{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "enhanced-reaction",
   "metadata": {},
   "source": [
    "# Iterative Pruning: _LeNet-300-100_ on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "defined-postcard",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explicit-sunset",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "constant-geneva",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.7.1\n"
     ]
    }
   ],
   "source": [
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shared-velvet",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "absent-panel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available device: cpu\n"
     ]
    }
   ],
   "source": [
    "# GPU device configuration-\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"Available device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proud-buffer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "expensive-parking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters-\n",
    "input_size = 784    # 28 x 28, flattened to be 1-D tensor\n",
    "hidden_size = 100\n",
    "num_classes = 10\n",
    "num_epochs = 20\n",
    "batch_size = 32\n",
    "learning_rate = 0.0012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-faith",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "patient-bruce",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/home/arjun/Documents/Programs/Python_Codes/PyTorch_Resources/Good_Codes/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "rotary-jacket",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset statistics:\n",
    "# mean = tensor([0.1307]) & std dev = tensor([0.3081])\n",
    "mean = np.array([0.1307])\n",
    "std_dev = np.array([0.3081])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "accredited-fleet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data set transformations to apply-\n",
    "transforms_apply = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean = mean, std = std_dev)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "meaningful-means",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset-\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "        root = './data', train = True,\n",
    "        transform = transforms_apply, download = True\n",
    "        )\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "        root = './data', train = False,\n",
    "        transform = transforms_apply\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cognitive-afghanistan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_dataset): 60000 & len(test_dataset): 10000\n"
     ]
    }
   ],
   "source": [
    "print(f\"len(train_dataset): {len(train_dataset)} & len(test_dataset): {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "lesbian-conditions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloader-\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        dataset = train_dataset, batch_size = batch_size,\n",
    "        shuffle = True\n",
    "        )\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        dataset = test_dataset, batch_size = batch_size,\n",
    "        shuffle = False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "billion-second",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_loader) = 1875 & len(test_loader) = 313\n"
     ]
    }
   ],
   "source": [
    "print(f\"len(train_loader) = {len(train_loader)} & len(test_loader) = {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "local-breed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "graduate-light",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 1, 28, 28]), torch.Size([32]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "images.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abroad-amplifier",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_samples.shape = torch.Size([32, 1, 28, 28]), labels.shape = torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "# Sanity check- one batch of data\n",
    "examples = iter(train_loader)\n",
    "\n",
    "# Unpack-\n",
    "img_samples, labels = examples.next()\n",
    "print(f\"img_samples.shape = {img_samples.shape}, labels.shape = {labels.shape}\")\n",
    "# We have '1' due to grey-scale images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "premium-preparation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# images[0][0].shape      # the second index '0' accesses the 1st channel\n",
    "# torch.Size([28, 28])\n",
    "\n",
    "# Visualize the digits-\n",
    "for i in range(6):\n",
    "    plt.subplot(2, 3, i + 1)    # 2 rows & 3 columns\n",
    "    plt.imshow(images[i][0], cmap = 'gray')     # '0' to access first channel\n",
    "    # plt.imshow(img_samples[i][0], cmap='gray')  # '0' to access first channel\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separated-lewis",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "filled-magic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.4242), tensor(2.8215))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check- Check whether transformations have been applied.\n",
    "# Look at first image out of 32 images-\n",
    "img_samples[0, :, :, :].min(), img_samples[0, :, :, :].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suburban-vulnerability",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confused-unemployment",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "guided-distribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet300(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet300, self).__init__()\n",
    "        \n",
    "        # Define layers-\n",
    "        self.fc1 = nn.Linear(in_features = input_size, out_features = 300)\n",
    "        self.fc2 = nn.Linear(in_features = 300, out_features = 100)\n",
    "        self.output = nn.Linear(in_features = 100, out_features = 10)\n",
    "        \n",
    "        self.weights_initialization()\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        return self.output(out)\n",
    "    \n",
    "    \n",
    "    def weights_initialization(self):\n",
    "        '''\n",
    "        When we define all the modules such as the layers in '__init__()'\n",
    "        method above, these are all stored in 'self.modules()'.\n",
    "        We go through each module one by one. This is the entire network,\n",
    "        basically.\n",
    "        '''\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weighted-genius",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "twenty-ecology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an instance of LeNet-300-100 dense neural network-\n",
    "model = LeNet300()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solar-virtue",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dated-mixture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer name: fc1.weight, has shape: torch.Size([300, 784])\n",
      "bias layer name: fc1.bias has shape: torch.Size([300])\n",
      "layer name: fc2.weight, has shape: torch.Size([100, 300])\n",
      "bias layer name: fc2.bias has shape: torch.Size([100])\n",
      "layer name: output.weight, has shape: torch.Size([10, 100])\n",
      "bias layer name: output.bias has shape: torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    # We do not prune bias term\n",
    "    if 'weight' in name:\n",
    "        print(f\"layer name: {name}, has shape: {param.shape}\")\n",
    "    elif 'bias' in name:\n",
    "        print(f\"bias layer name: {name} has shape: {param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conceptual-mechanics",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "buried-worst",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss and optimizer-\n",
    "loss = nn.CrossEntropyLoss()    # applies softmax for us\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competitive-homeless",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print optimizer's state_dict\n",
    "print(\"Optimizer's 'state_dict':\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(f\"var_name: {var_name} \\t {optimizer.state_dict()[var_name]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "registered-container",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "explicit-there",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_steps = 1875 & len(train_dataset)/batch_size = 1875.0\n",
      "number of training steps in one epoch = 1875\n"
     ]
    }
   ],
   "source": [
    "# Training loop-\n",
    "num_steps = len(train_loader)\n",
    "\n",
    "print(f\"num_steps = {num_steps} & len(train_dataset)/batch_size = {len(train_dataset) / batch_size}\")\n",
    "print(f\"number of training steps in one epoch = {num_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auburn-ukraine",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "objective-pearl",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_params(model):\n",
    "    \n",
    "    tot_params = 0\n",
    "    for layer_name, param in model.named_parameters():\n",
    "        # print(f\"{layer_name}.shape = {param.shape} has {torch.count_nonzero(param.data)} non-zero params\")\n",
    "        tot_params += torch.count_nonzero(param.data)\n",
    "    \n",
    "    return tot_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "macro-titanium",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minute-evolution",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southeast-diesel",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worldwide-correction",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "leading-paintball",
   "metadata": {},
   "source": [
    "### Train defined model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "stuffed-andrews",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User input parameters for Early Stopping in manual implementation-\n",
    "minimum_delta = 0.001\n",
    "patience = 5\n",
    "\n",
    "# Initialize parameters for Early Stopping manual implementation-\n",
    "best_val_loss = 100\n",
    "loc_patience = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "happy-istanbul",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python3 lists to store model training metrics-\n",
    "training_acc = []\n",
    "validation_acc = []\n",
    "training_loss = []\n",
    "validation_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conservative-workshop",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "foreign-cooperation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_grad_freezing(model, epoch):\n",
    "    '''\n",
    "    Function to train one epoch of training dataset.\n",
    "    '''\n",
    "        \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "        \n",
    "    for batch, (images, labels) in enumerate(train_loader):\n",
    "        # Reshape images first-\n",
    "        # 32, 1, 28, 28\n",
    "        # Input size needs to be 32, 784-\n",
    "        images = images.reshape(-1, 28 * 28 * 1).to(device)\n",
    "        # Tries to push to GPU if available\n",
    "        labels = labels.to(device)\n",
    "        # images, labels = images.reshape(-1, 28 * 28 * 1).to(device), labels.to(device)\n",
    "        \n",
    "        # Set defined model to training mode-\n",
    "        model.train()\n",
    "        \n",
    "        # Backward pass-\n",
    "        optimizer.zero_grad()   # empty accumulated gradients\n",
    "\n",
    "        # Forward pass-\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Compute loss-\n",
    "        J = loss(outputs, labels)\n",
    "        \n",
    "        # Perform backpropagation-\n",
    "        J.backward()\n",
    "        \n",
    "        # Freezing Pruned weights by making their gradients Zero\n",
    "        for layer_name, param in model.named_parameters():\n",
    "            if 'weight' in layer_name:\n",
    "                tensor = param.data.cpu().numpy()\n",
    "                grad_tensor = param.grad.data.cpu().numpy()\n",
    "                # grad_tensor = np.where(tensor < EPS, 0, grad_tensor)\n",
    "                grad_tensor = np.where(tensor == 0, 0, grad_tensor)\n",
    "                param.grad.data = torch.from_numpy(grad_tensor).to(device)\n",
    "\n",
    "        # Update parameters-\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Compute model's performance statistics-\n",
    "        running_loss += J.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        running_corrects += torch.sum(predicted == labels.data)\n",
    "\n",
    "        '''\n",
    "        # Print information every 100 steps-\n",
    "        if (batch + 1) % 100 == 0:\n",
    "            print(f\"epoch {epoch + 1}/{num_epochs}, step {batch + 1}/{num_steps}, loss = {J.item():.4f}\")\n",
    "        '''\n",
    "        \n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    epoch_acc = running_corrects.double() / len(train_dataset)\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "killing-surveillance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "spread-lodge",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, epoch):\n",
    "    '''\n",
    "    Function to validate performance of trained 'model' on testing set.\n",
    "    '''\n",
    "    \n",
    "    running_loss_val = 0.0\n",
    "\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "\n",
    "            # Place features (images) and targets (labels) to GPU-\n",
    "            # images = images.to(device)\n",
    "            images = images.reshape(-1, 28 * 28 * 1).to(device)\n",
    "            labels = labels.to(device)\n",
    "            # images, labels = images.reshape(-1, 28 * 28 * 1).to(device), targets.to(device)\n",
    "            # print(f\"images.shape = {images.shape}, labels.shape = {labels.shape}\")\n",
    "            \n",
    "            # Set model to evaluation mode-\n",
    "            model.eval()\n",
    "    \n",
    "            # Make predictions using trained model-\n",
    "            outputs = model(images)\n",
    "            _, y_pred = torch.max(outputs, 1)\n",
    "\n",
    "            # Compute validation loss-\n",
    "            J_val = loss(outputs, labels)\n",
    "\n",
    "            running_loss_val += J_val.item() * labels.size(0)\n",
    "    \n",
    "            # Total number of labels-\n",
    "            total += labels.size(0)\n",
    "\n",
    "            # Total number of correct predictions-\n",
    "            correct += (y_pred == labels).sum()\n",
    "\n",
    "    epoch_val_loss = running_loss_val / len(test_dataset)\n",
    "    val_acc = 100 * (correct / total)\n",
    "    \n",
    "    return epoch_val_loss, val_acc\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authentic-vision",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arbitrary-heritage",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "collectible-swing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 1, # of params = 266610, training loss = 0.2036, training accuracy = 93.77%, val_loss = 0.1190 & val_accuracy = 96.21%\n",
      "\n",
      "\n",
      "Saving model with lowest val_loss = 0.1190\n",
      "\n",
      "epoch: 2, # of params = 266610, training loss = 0.0968, training accuracy = 97.04%, val_loss = 0.1003 & val_accuracy = 96.91%\n",
      "\n",
      "\n",
      "Saving model with lowest val_loss = 0.1003\n",
      "\n",
      "epoch: 3, # of params = 266610, training loss = 0.0738, training accuracy = 97.65%, val_loss = 0.1087 & val_accuracy = 96.67%\n",
      "\n",
      "\n",
      "epoch: 4, # of params = 266610, training loss = 0.0608, training accuracy = 98.06%, val_loss = 0.0971 & val_accuracy = 97.31%\n",
      "\n",
      "\n",
      "Saving model with lowest val_loss = 0.0971\n",
      "\n",
      "epoch: 5, # of params = 266610, training loss = 0.0523, training accuracy = 98.36%, val_loss = 0.0992 & val_accuracy = 97.24%\n",
      "\n",
      "\n",
      "epoch: 6, # of params = 266610, training loss = 0.0419, training accuracy = 98.65%, val_loss = 0.0917 & val_accuracy = 97.65%\n",
      "\n",
      "\n",
      "Saving model with lowest val_loss = 0.0917\n",
      "\n",
      "epoch: 7, # of params = 266610, training loss = 0.0387, training accuracy = 98.79%, val_loss = 0.0983 & val_accuracy = 97.59%\n",
      "\n",
      "\n",
      "epoch: 8, # of params = 266610, training loss = 0.0344, training accuracy = 98.92%, val_loss = 0.1141 & val_accuracy = 97.47%\n",
      "\n",
      "\n",
      "epoch: 9, # of params = 266610, training loss = 0.0332, training accuracy = 98.97%, val_loss = 0.1070 & val_accuracy = 97.84%\n",
      "\n",
      "\n",
      "epoch: 10, # of params = 266610, training loss = 0.0300, training accuracy = 99.11%, val_loss = 0.1006 & val_accuracy = 97.86%\n",
      "\n",
      "\n",
      "epoch: 11, # of params = 266610, training loss = 0.0295, training accuracy = 99.09%, val_loss = 0.1223 & val_accuracy = 97.52%\n",
      "\n",
      "\n",
      "\n",
      "Early stopping called. Exiting model training!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop-\n",
    "for curr_epoch in range(1, num_epochs):\n",
    "    \n",
    "    if loc_patience >= patience:\n",
    "        print(\"\\n\\nEarly stopping called. Exiting model training!\\n\\n\")\n",
    "        break\n",
    "        \n",
    "    # epoch_loss, epoch_acc = train(model = model, epoch = curr_epoch)\n",
    "    epoch_loss, epoch_acc = train_with_grad_freezing(model = model, epoch = curr_epoch)\n",
    "    epoch_val_loss, val_acc = test(model = model, epoch = curr_epoch)\n",
    "    \n",
    "    remaining_params = count_params(model)\n",
    "    # Pruned LeNet-300-100 model has 226730 trainable parameters\n",
    "    \n",
    "    print(f\"\\nepoch: {curr_epoch}, # of params = {remaining_params}, training loss = {epoch_loss:.4f}, training accuracy = {epoch_acc * 100:.2f}%, val_loss = {epoch_val_loss:.4f} & val_accuracy = {val_acc:.2f}%\\n\")    \n",
    "    # print(f\"\\nepoch: {curr_epoch} training loss = {epoch_loss:.4f}, training accuracy = {epoch_acc * 100:.2f}%, val_loss = {epoch_val_loss:.4f} & val_accuracy = {val_acc:.2f}%\\n\")\n",
    "\n",
    "    \n",
    "    # Code for manual Early Stopping:\n",
    "    # if np.abs(epoch_val_loss < best_val_loss) >= minimum_delta:\n",
    "    if (epoch_val_loss < best_val_loss) and np.abs(epoch_val_loss - best_val_loss) >= minimum_delta:\n",
    "        # print(f\"epoch_val_loss = {epoch_val_loss:.4f}, best_val_loss = {best_val_loss:.4f}\")\n",
    "        \n",
    "        # update 'best_val_loss' variable to lowest loss encountered so far-\n",
    "        best_val_loss = epoch_val_loss\n",
    "        \n",
    "        # reset 'loc_patience' variable-\n",
    "        loc_patience = 0\n",
    "        \n",
    "        print(f\"\\nSaving model with lowest val_loss = {epoch_val_loss:.4f}\")\n",
    "        \n",
    "        # Save trained model with validation accuracy-\n",
    "        # torch.save(model.state_dict, f\"LeNet-300-100_Trained_{val_acc}.pth\")\n",
    "        torch.save(model.state_dict(), \"LeNet-300-100_Trained.pth\")\n",
    "        \n",
    "    else:  # there is no improvement in monitored metric 'val_loss'\n",
    "        loc_patience += 1  # number of epochs without any improvement\n",
    "\n",
    "\n",
    "    training_acc.append(epoch_acc * 100)\n",
    "    validation_acc.append(val_acc)\n",
    "    training_loss.append(epoch_loss)\n",
    "    validation_loss.append(epoch_val_loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empirical-swift",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "declared-thursday",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/home/arjun/Deep_Learning_Resources/LTH-Resources/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "published-compiler",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a new model for best weights achieved during training-\n",
    "best_model = LeNet300()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "preceding-receipt",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.load_state_dict(torch.load('LeNet-300-100_Trained.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indie-provision",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "exceptional-poverty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute trained model's metrics on validation data-\n",
    "val_loss, val_acc = test(model = best_model, epoch = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "precious-highlight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LeNet-300-100 trained model metrics:\n",
      "val_loss = 0.0917 & val_accuracy = 97.65%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nLeNet-300-100 trained model metrics:\")\n",
    "print(f\"val_loss = {val_loss:.4f} & val_accuracy = {val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infectious-syndication",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "korean-raising",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete 'model' since it's performance degraded due to 'patience' which led to over-fitting-\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-league",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empirical-renewal",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "incoming-projection",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_lenet(model, pruning_params_fc, pruning_params_op):\n",
    "    '''\n",
    "    Function to prune top p% of trained weights using the provided parameters using\n",
    "    magnitude-based weight pruning.\n",
    "    \n",
    "    Inputs:\n",
    "    'model' is the PyTorch 1.7 defined neural network\n",
    "    'pruning_params_fc' is the percentage of weights to prune for dense, fully-connected layer\n",
    "    'pruning_params_op' is the percentage of weights to prune for output layer\n",
    "\n",
    "    Returns:\n",
    "    Python dict containing pruned layers\n",
    "    '''\n",
    "    \n",
    "    # Python3 dict to hold pruned weights-\n",
    "    pruned_d = {}\n",
    "    \n",
    "    # Sample code- populate each layer with relevant weights-\n",
    "    for layer_name, param in best_model.named_parameters():\n",
    "        # pruned_d[layer_name] = torch.zeros_like(param.data)\n",
    "        x = param.data.numpy()\n",
    "    \n",
    "        if len(x.shape) == 2 and x.shape[0] != 10:\n",
    "            # FC layer-\n",
    "            # print(layer_name, param.shape)\n",
    "    \n",
    "            # Compute absolute value of 'x'-\n",
    "            x_abs = np.abs(x)\n",
    "\n",
    "            # Mask values to zero which are less than 'p' in terms of magnitude-\n",
    "            x_abs[x_abs < np.percentile(x_abs, pruning_params_fc)] = 0\n",
    "\n",
    "            # Where 'x_abs' equals 0, keep 0, else, replace with values of 'x'-\n",
    "            # OR\n",
    "            # If x_abs == 0 (condition) is True, use the value of 0, otherwise\n",
    "            # use the value in 'x'\n",
    "            x_mod = np.where(x_abs == 0, 0, x)\n",
    "    \n",
    "            # Counts the number of non-zero values in the array 'x_mod'-\n",
    "            # np.count_nonzero(x_mod)\n",
    "    \n",
    "            # pruned_weights.append(x_mod)\n",
    "            pruned_d[layer_name] = torch.from_numpy(x_mod)\n",
    "    \n",
    "        elif len(x.shape) == 2 and x.shape[0] == 10:\n",
    "            # print(\"output layer\", param.shape)\n",
    "        \n",
    "            # Output layer-\n",
    "            # print(layer_name, param.shape)\n",
    "    \n",
    "            # Compute absolute value of 'x'-\n",
    "            x_abs = np.abs(x)\n",
    "\n",
    "            # Mask values to zero which are less than 'p' in terms of magnitude-\n",
    "            x_abs[x_abs < np.percentile(x_abs, pruning_params_op)] = 0\n",
    "\n",
    "            # Where 'x_abs' equals 0, keep 0, else, replace with values of 'x'-\n",
    "            # OR\n",
    "            # If x_abs == 0 (condition) is True, use the value of 0, otherwise\n",
    "            # use the value in 'x'\n",
    "            x_mod = np.where(x_abs == 0, 0, x)\n",
    "    \n",
    "            # Counts the number of non-zero values in the array 'x_mod'-\n",
    "            # np.count_nonzero(x_mod)\n",
    "    \n",
    "            # pruned_weights.append(x_mod)\n",
    "            pruned_d[layer_name] = torch.from_numpy(x_mod)\n",
    "    \n",
    "        else:\n",
    "            pruned_d[layer_name] = param.data\n",
    "\n",
    "    \n",
    "    return pruned_d\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "psychological-deposit",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "periodic-commercial",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prune 15% of smallest magnitude weights in FC layers and 10% in output layer-\n",
    "pruned_d = prune_lenet(model = best_model, pruning_params_fc = 15, pruning_params_op = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "banner-narrative",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pruned_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thorough-equation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "humanitarian-scott",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize and load pruned Python3 dict into a new model-\n",
    "pruned_model = LeNet300()\n",
    "pruned_model.load_state_dict(pruned_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepting-ancient",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "handed-arthritis",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_pruned = count_params(pruned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "innovative-wound",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of non-zero parameters in pruned model = 226730\n"
     ]
    }
   ],
   "source": [
    "print(f\"# of non-zero parameters in pruned model = {params_pruned.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conscious-space",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bridal-player",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arabic-algeria",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gorgeous-strand",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "frank-convenience",
   "metadata": {},
   "source": [
    "### Re-train pruned model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fiscal-medline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User input parameters for Early Stopping in manual implementation-\n",
    "minimum_delta = 0.001\n",
    "patience = 5\n",
    "\n",
    "# Initialize parameters for Early Stopping manual implementation-\n",
    "best_val_loss = 100\n",
    "loc_patience = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "indian-keyboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python3 lists to store model training metrics-\n",
    "training_acc = []\n",
    "validation_acc = []\n",
    "training_loss = []\n",
    "validation_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "photographic-tutorial",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "lovely-playback",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 1, # of params = 226730, training loss = 0.0285, training accuracy = 99.04%, val_loss = 0.0910 & val_accuracy = 97.68%\n",
      "\n",
      "\n",
      "Saving model with lowest val_loss = 0.0910\n",
      "\n",
      "epoch: 2, # of params = 226730, training loss = 0.0285, training accuracy = 99.04%, val_loss = 0.0910 & val_accuracy = 97.68%\n",
      "\n",
      "\n",
      "epoch: 3, # of params = 226730, training loss = 0.0285, training accuracy = 99.04%, val_loss = 0.0910 & val_accuracy = 97.68%\n",
      "\n",
      "\n",
      "epoch: 4, # of params = 226730, training loss = 0.0285, training accuracy = 99.04%, val_loss = 0.0910 & val_accuracy = 97.68%\n",
      "\n",
      "\n",
      "epoch: 5, # of params = 226730, training loss = 0.0285, training accuracy = 99.04%, val_loss = 0.0910 & val_accuracy = 97.68%\n",
      "\n",
      "\n",
      "epoch: 6, # of params = 226730, training loss = 0.0285, training accuracy = 99.04%, val_loss = 0.0910 & val_accuracy = 97.68%\n",
      "\n",
      "\n",
      "\n",
      "Early stopping called. Exiting model training!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop-\n",
    "for curr_epoch in range(1, num_epochs):\n",
    "    \n",
    "    if loc_patience >= patience:\n",
    "        print(\"\\n\\nEarly stopping called. Exiting model training!\\n\\n\")\n",
    "        break\n",
    "        \n",
    "    # epoch_loss, epoch_acc = train(model = model, epoch = curr_epoch)\n",
    "    epoch_loss, epoch_acc = train_with_grad_freezing(model = pruned_model, epoch = curr_epoch)\n",
    "    epoch_val_loss, val_acc = test(model = pruned_model, epoch = curr_epoch)\n",
    "    \n",
    "    remaining_params = count_params(pruned_model)\n",
    "    \n",
    "    print(f\"\\nepoch: {curr_epoch}, # of params = {remaining_params}, training loss = {epoch_loss:.4f}, training accuracy = {epoch_acc * 100:.2f}%, val_loss = {epoch_val_loss:.4f} & val_accuracy = {val_acc:.2f}%\\n\")    \n",
    "    # print(f\"\\nepoch: {curr_epoch} training loss = {epoch_loss:.4f}, training accuracy = {epoch_acc * 100:.2f}%, val_loss = {epoch_val_loss:.4f} & val_accuracy = {val_acc:.2f}%\\n\")\n",
    "\n",
    "    \n",
    "    # Code for manual Early Stopping:\n",
    "    # if np.abs(epoch_val_loss < best_val_loss) >= minimum_delta:\n",
    "    if (epoch_val_loss < best_val_loss) and np.abs(epoch_val_loss - best_val_loss) >= minimum_delta:\n",
    "        # print(f\"epoch_val_loss = {epoch_val_loss:.4f}, best_val_loss = {best_val_loss:.4f}\")\n",
    "        \n",
    "        # update 'best_val_loss' variable to lowest loss encountered so far-\n",
    "        best_val_loss = epoch_val_loss\n",
    "        \n",
    "        # reset 'loc_patience' variable-\n",
    "        loc_patience = 0\n",
    "        \n",
    "        print(f\"\\nSaving model with lowest val_loss = {epoch_val_loss:.4f}\")\n",
    "        \n",
    "        # Save trained model with validation accuracy-\n",
    "        # torch.save(model.state_dict, f\"LeNet-300-100_Trained_{val_acc}.pth\")\n",
    "        torch.save(pruned_model.state_dict(), \"LeNet-300-100_Test_Trained.pth\")\n",
    "        \n",
    "    else:  # there is no improvement in monitored metric 'val_loss'\n",
    "        loc_patience += 1  # number of epochs without any improvement\n",
    "\n",
    "\n",
    "    training_acc.append(epoch_acc * 100)\n",
    "    validation_acc.append(val_acc)\n",
    "    training_loss.append(epoch_loss)\n",
    "    validation_loss.append(epoch_val_loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exceptional-tissue",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beginning-singing",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
